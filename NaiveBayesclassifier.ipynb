{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55abc2a",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb29548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # 导入sklearn package\n",
    "news = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac634c94",
   "metadata": {},
   "source": [
    "# 划分训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cc0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25) # 划分训练集与测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4e1b3",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "创建wordlist\n",
    "* 全部转化为小写\n",
    "* 去掉标点符号\n",
    "* 去掉换行符\n",
    "* 将数据以空格划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274746a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "word_set = set()\n",
    "counter_per_cat = [Counter() for i in range(len(news.target_names))]  # 创建文章类别个数的Counter()，计算每个单词在所有文档类别出现的频率 \n",
    "for sample_id, sample in enumerate(x_train):\n",
    "    sample_label = y_train[sample_id]\n",
    "    words = str(sample).lower().translate(table).strip().split()#lower()转化为小写 translate()去掉标点符号 strip()去掉换行符 split()将数据以空格划分\n",
    "    counter_per_cat[sample_label].update(words)\n",
    "    word_set.update(words)\n",
    "word_list = list(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ad0e8",
   "metadata": {},
   "source": [
    "# Classifier learning\n",
    "* prob_mat: 类条件概率\n",
    "    * size of[label_num（类别数）, word_num（Vocabulary大小）]\n",
    "    * 类条件概率\n",
    "* Get expected statistics from the training data\n",
    "    * total_freq : total word frequency in articles with certain label（具有特定标签的文章的总词频）\n",
    "    * label_prob : the class prior probabilities of training data（先验概率）\n",
    "    * empty_prob : for word doesn’t appear in training data（处理没有出现在训练数据中的词）\n",
    "    * prob_mat : probability of each word appears in each article label\n",
    "* Use these statistics to classify articles in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf86e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_freq = [] # 每类文章的总词频：nj\n",
    "label_prob = [] # 先验概率\n",
    "empty_prob = [] # 没有出现在训练数据中的词\n",
    "prior_prob = Counter(y_train)\n",
    "for label_id, label in enumerate(news.target_names):\n",
    "    total_freq.append(sum(counter_per_cat[label_id].values())) # 每类文章的总词频：nj\n",
    "    label_prob.append(prior_prob[label_id]/len(y_train)) # 先验概率\n",
    "    empty_prob.append((1)/(total_freq[label_id] + len(word_list))) # 没有出现在训练数据中的词\n",
    "\n",
    "import numpy as np\n",
    "prob_mat = np.zeros((len(news.target_names), len(word_set))) # 类条件概率\n",
    "for label_id, label in enumerate(news.target_names):\n",
    "    freq_list = counter_per_cat[label_id]\n",
    "    for word_id, word in enumerate(word_set):\n",
    "        if word not in freq_list:\n",
    "            freq_label = 0\n",
    "        else:\n",
    "            freq_label = freq_list[word]\n",
    "        freq_all = total_freq[label_id]\n",
    "        prob = (freq_label + 1) / (freq_all + len(word_set)) # 计算类条件概率\n",
    "        prob_mat[label_id, word_id] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05fe22",
   "metadata": {},
   "source": [
    "# News article classification\n",
    "* The steps for testing:\n",
    "    * Tokenize the articles in test data (数据预处理)\n",
    "    * Calculate the probability of the article belongs to each label（计算文章属于每一类的概率）\n",
    "    * Classify it with the highest one（将文章分类为上面计算概率最大的类）\n",
    "* Issue:\n",
    "    * To avoid the product of probabilities getting too close to0, we use log likelihood equation: Convert product to addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                         | 238/4712 [21:04<8:37:40,  6.94s/it]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "label_prob = np.log(np.array(label_prob))\n",
    "empty_prob = np.log(np.array(empty_prob))\n",
    "prob_mat = np.log(np.array(prob_mat))\n",
    "predict_labels = np.zeros(len(x_test), dtype = int)\n",
    "for sample_id, sample in tqdm(enumerate(x_test), total = len(x_test)):\n",
    "    probs = np.zeros(len(news.target_names))\n",
    "    words = str(sample).lower().translate(table).strip().split() # Tokenize\n",
    "    sample_len = len(words)\n",
    "    word_freq = Counter(words)\n",
    "    words = list(set(words).intersection(set(word_list))) # 返回words和word_list的交集\n",
    "    for label_id, label in enumerate(news.target_names): # Calculate the probality for each category\n",
    "        prob_label = label_prob[label_id]\n",
    "        len_a = 0\n",
    "        for word in words:\n",
    "            word_id = word_list.index(word)\n",
    "            prob_cur = prob_mat[label_id, word_id]\n",
    "            prob_label += word_freq[word] * (prob_cur)\n",
    "            len_a += word_freq[word]\n",
    "        len_b = sample_len - len_a\n",
    "        if len_b > 0:\n",
    "            prob_label += len_b * (empty_prob[label_id])\n",
    "        probs[label_id] = prob_label\n",
    "    predict_label = np.argmax(probs) # Category with the highest probality\n",
    "    predict_labels[sample_id] = predict_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b7bb9",
   "metadata": {},
   "source": [
    "# Results\n",
    "* Calculate the accuracy of the prediction among all of the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the prediction among all of the categories\n",
    "comp = predict_labels - np.array(y_test)\n",
    "accuracy = (len(np.where(comp == 0)[0])) / len(x_test)\n",
    "print(accuracy)\n",
    "\n",
    "# Detailed accuracy\n",
    "y_test = np.array(y_test)\n",
    "for i in range(len(news.target_names)):\n",
    "    cat_data_num = (np.where(y_test == i))[0].shape[0]\n",
    "    pred_cat_tmp = predict_labels[np.where(y_test == i)]\n",
    "    pred_correct = np.where(pred_cat_tmp == i)[0].shape[0]\n",
    "    accuracy_per_cat = pred_correct/cat_data_num\n",
    "    print(news.target_names[i])\n",
    "    print(\"total data number of this category: \" + str(cat_data_num))\n",
    "    print(\"number of correctly prediction: \" + str(pred_correct))\n",
    "    print(\"predicition accuracy for this category: \" + str(accuracy_per_cat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
