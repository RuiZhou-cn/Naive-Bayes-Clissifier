{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfc553a",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30831231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # 导入sklearn package\n",
    "news = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc16db",
   "metadata": {},
   "source": [
    "# 划分训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a63b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25) # 划分训练集与测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc707fd0",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "创建wordlist\n",
    "* 全部转化为小写\n",
    "* 去掉标点符号\n",
    "* 去掉换行符\n",
    "* 将数据以空格划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e3feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "word_set = set()\n",
    "counter_per_cat = [Counter() for i in range(len(news.target_names))]  # 创建文章类别个数的Counter()，计算每个单词在所有文档类别出现的频率 \n",
    "for sample_id, sample in enumerate(x_train):\n",
    "    sample_label = y_train[sample_id]\n",
    "    words = str(sample).lower().translate(table).strip().split()#lower()转化为小写 translate()去掉标点符号 strip()去掉换行符 split()将数据以空格划分\n",
    "    counter_per_cat[sample_label].update(words)\n",
    "    word_set.update(words)\n",
    "word_list = list(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e49ba",
   "metadata": {},
   "source": [
    "# Classifier learning\n",
    "* prob_mat: 类条件概率\n",
    "    * size of[label_num（类别数）, word_num（Vocabulary大小）]\n",
    "    * 类条件概率\n",
    "* Get expected statistics from the training data\n",
    "    * total_freq : total word frequency in articles with certain label（具有特定标签的文章的总词频）\n",
    "    * label_prob : the class prior probabilities of training data（先验概率）\n",
    "    * empty_prob : for word doesn’t appear in training data（处理没有出现在训练数据中的词）\n",
    "    * prob_mat : probability of each word appears in each article label\n",
    "* Use these statistics to classify articles in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02c4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_freq = [] # 每类文章的总词频：nj\n",
    "label_prob = [] # 先验概率\n",
    "empty_prob = [] # 没有出现在训练数据中的词\n",
    "prior_prob = Counter(y_train)\n",
    "for label_id, label in enumerate(news.target_names):\n",
    "    total_freq.append(sum(counter_per_cat[label_id].values())) # 每类文章的总词频：nj\n",
    "    label_prob.append(prior_prob[label_id]/len(y_train)) # 先验概率\n",
    "    empty_prob.append((1)/(total_freq[label_id] + len(word_list))) # 没有出现在训练数据中的词\n",
    "\n",
    "import numpy as np\n",
    "prob_mat = np.zeros((len(news.target_names), len(word_set))) # 类条件概率\n",
    "for label_id, label in enumerate(news.target_names):\n",
    "    freq_list = counter_per_cat[label_id]\n",
    "    for word_id, word in enumerate(word_set):\n",
    "        if word not in freq_list:\n",
    "            freq_label = 0\n",
    "        else:\n",
    "            freq_label = freq_list[word]\n",
    "        freq_all = total_freq[label_id]\n",
    "        prob = (freq_label + 1) / (freq_all + len(word_set)) # 计算类条件概率\n",
    "        prob_mat[label_id, word_id] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f020a47",
   "metadata": {},
   "source": [
    "# News article classification\n",
    "* The steps for testing:\n",
    "    * Tokenize the articles in test data (数据预处理)\n",
    "    * Calculate the probability of the article belongs to each label（计算文章属于每一类的概率）\n",
    "    * Classify it with the highest one（将文章分类为上面计算概率最大的类）\n",
    "* Issue:\n",
    "    * To avoid the product of probabilities getting too close to0, we use log likelihood equation: Convert product to addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f4cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4712/4712 [5:36:08<00:00,  4.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "label_prob = np.log(np.array(label_prob))\n",
    "empty_prob = np.log(np.array(empty_prob))\n",
    "prob_mat = np.log(np.array(prob_mat))\n",
    "predict_labels = np.zeros(len(x_test), dtype = int)\n",
    "for sample_id, sample in tqdm(enumerate(x_test), total = len(x_test)):\n",
    "    probs = np.zeros(len(news.target_names))\n",
    "    words = str(sample).lower().translate(table).strip().split() # Tokenize\n",
    "    sample_len = len(words)\n",
    "    word_freq = Counter(words)\n",
    "    words = list(set(words).intersection(set(word_list))) # 返回words和word_list的交集\n",
    "    for label_id, label in enumerate(news.target_names): # Calculate the probality for each category\n",
    "        prob_label = label_prob[label_id]\n",
    "        len_a = 0\n",
    "        for word in words:\n",
    "            word_id = word_list.index(word)\n",
    "            prob_cur = prob_mat[label_id, word_id]\n",
    "            prob_label += word_freq[word] * (prob_cur)\n",
    "            len_a += word_freq[word]\n",
    "        len_b = sample_len - len_a\n",
    "        if len_b > 0:\n",
    "            prob_label += len_b * (empty_prob[label_id])\n",
    "        probs[label_id] = prob_label\n",
    "    predict_label = np.argmax(probs) # Category with the highest probality\n",
    "    predict_labels[sample_id] = predict_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4b964",
   "metadata": {},
   "source": [
    "# Results\n",
    "* Calculate the accuracy of the prediction among all of the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d378e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8718166383701188\n",
      "alt.atheism\n",
      "total data number of this category: 208\n",
      "number of correctly prediction: 186\n",
      "predicition accuracy for this category: 0.8942307692307693\n",
      "comp.graphics\n",
      "total data number of this category: 231\n",
      "number of correctly prediction: 201\n",
      "predicition accuracy for this category: 0.8701298701298701\n",
      "comp.os.ms-windows.misc\n",
      "total data number of this category: 249\n",
      "number of correctly prediction: 173\n",
      "predicition accuracy for this category: 0.6947791164658634\n",
      "comp.sys.ibm.pc.hardware\n",
      "total data number of this category: 217\n",
      "number of correctly prediction: 181\n",
      "predicition accuracy for this category: 0.8341013824884793\n",
      "comp.sys.mac.hardware\n",
      "total data number of this category: 250\n",
      "number of correctly prediction: 213\n",
      "predicition accuracy for this category: 0.852\n",
      "comp.windows.x\n",
      "total data number of this category: 257\n",
      "number of correctly prediction: 232\n",
      "predicition accuracy for this category: 0.9027237354085603\n",
      "misc.forsale\n",
      "total data number of this category: 252\n",
      "number of correctly prediction: 156\n",
      "predicition accuracy for this category: 0.6190476190476191\n",
      "rec.autos\n",
      "total data number of this category: 248\n",
      "number of correctly prediction: 226\n",
      "predicition accuracy for this category: 0.9112903225806451\n",
      "rec.motorcycles\n",
      "total data number of this category: 253\n",
      "number of correctly prediction: 240\n",
      "predicition accuracy for this category: 0.9486166007905138\n",
      "rec.sport.baseball\n",
      "total data number of this category: 240\n",
      "number of correctly prediction: 229\n",
      "predicition accuracy for this category: 0.9541666666666667\n",
      "rec.sport.hockey\n",
      "total data number of this category: 264\n",
      "number of correctly prediction: 255\n",
      "predicition accuracy for this category: 0.9659090909090909\n",
      "sci.crypt\n",
      "total data number of this category: 251\n",
      "number of correctly prediction: 243\n",
      "predicition accuracy for this category: 0.9681274900398407\n",
      "sci.electronics\n",
      "total data number of this category: 230\n",
      "number of correctly prediction: 191\n",
      "predicition accuracy for this category: 0.8304347826086956\n",
      "sci.med\n",
      "total data number of this category: 225\n",
      "number of correctly prediction: 211\n",
      "predicition accuracy for this category: 0.9377777777777778\n",
      "sci.space\n",
      "total data number of this category: 253\n",
      "number of correctly prediction: 233\n",
      "predicition accuracy for this category: 0.9209486166007905\n",
      "soc.religion.christian\n",
      "total data number of this category: 262\n",
      "number of correctly prediction: 252\n",
      "predicition accuracy for this category: 0.9618320610687023\n",
      "talk.politics.guns\n",
      "total data number of this category: 240\n",
      "number of correctly prediction: 223\n",
      "predicition accuracy for this category: 0.9291666666666667\n",
      "talk.politics.mideast\n",
      "total data number of this category: 224\n",
      "number of correctly prediction: 220\n",
      "predicition accuracy for this category: 0.9821428571428571\n",
      "talk.politics.misc\n",
      "total data number of this category: 201\n",
      "number of correctly prediction: 183\n",
      "predicition accuracy for this category: 0.9104477611940298\n",
      "talk.religion.misc\n",
      "total data number of this category: 157\n",
      "number of correctly prediction: 60\n",
      "predicition accuracy for this category: 0.3821656050955414\n"
     ]
    }
   ],
   "source": [
    "# accuracy of the prediction among all of the categories\n",
    "comp = predict_labels - np.array(y_test)\n",
    "accuracy = (len(np.where(comp == 0)[0])) / len(x_test)\n",
    "print(accuracy)\n",
    "\n",
    "# Detailed accuracy\n",
    "y_test = np.array(y_test)\n",
    "for i in range(len(news.target_names)):\n",
    "    cat_data_num = (np.where(y_test == i))[0].shape[0]\n",
    "    pred_cat_tmp = predict_labels[np.where(y_test == i)]\n",
    "    pred_correct = np.where(pred_cat_tmp == i)[0].shape[0]\n",
    "    accuracy_per_cat = pred_correct/cat_data_num\n",
    "    print(news.target_names[i])\n",
    "    print(\"total data number of this category: \" + str(cat_data_num))\n",
    "    print(\"number of correctly prediction: \" + str(pred_correct))\n",
    "    print(\"predicition accuracy for this category: \" + str(accuracy_per_cat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
